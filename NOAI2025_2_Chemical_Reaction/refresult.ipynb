{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca377551-5d06-4ca3-bc74-e37d81385ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e9b4a-fdd1-408e-9379-98cab2b61858",
   "metadata": {},
   "source": [
    "## Method 1: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4a4e4a-8e81-4f5a-90ef-0530f50f502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd3c616-2688-47ad-ba6a-aaa1980dceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练数据\n",
    "# train_df = pd.read_csv('/bohr/train-lmss/v1/data_train/training_data.dat')  # 根据实际分隔符调整\n",
    "train_df = pd.read_csv('./data/data_train/training_data.dat')\n",
    "\n",
    "L2M = train_df.iloc[:, 1].values\n",
    "D = train_df.iloc[:, 2].values\n",
    "L = train_df.iloc[:, 3].values\n",
    "\n",
    "train_df['c(L2M)/c(D)'] = L2M / D\n",
    "train_df['c(L)/c(D)'] = L / D\n",
    "train_df['c(L)/c(L2M)'] = L / L2M\n",
    "train_df['c(L2M)*c(D)'] = L2M * D\n",
    "train_df['c(L)*c(D)'] = L * D\n",
    "train_df['c(L)*c(L2M)'] = L * L2M\n",
    "train_df['L2M_mole'] = L2M / (L2M + D + L)\n",
    "train_df['D_mole'] = D / (L2M + D + L)\n",
    "train_df['L_mole'] = L / (L2M + D + L)\n",
    "train_df['logc(L2M)'] = np.log(1+L2M)\n",
    "\n",
    "# 定义输入特征和标签\n",
    "features = [1,2,3,9,10,11,12,13,14,15,16,17,18]\n",
    "target = 5\n",
    "\n",
    "X = train_df.iloc[:, features].values\n",
    "y = train_df.iloc[:, target].values\n",
    "\n",
    "# 数据归一化（可选，XGBoost对特征尺度不敏感，但可加速收敛）\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926f9c9-9b86-41a3-8389-1756f67a2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3225f0-80f3-47d7-a2eb-ddd5371a0d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [4, 6, 8, 10, 12],\n",
    "    \"learning_rate\": [0.001, 0.005, 0.01, 0.05, 0.1, 0.5],\n",
    "    \"n_estimators\": [200, 500, 1000, 1500, 2000, 3000, 5000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb.XGBRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_scaled, y)\n",
    "print(\"Best Params:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a084d-7e22-4536-a921-92b4e48f5a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化XGBoost回归模型\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=200,     # 树的数量\n",
    "    learning_rate=0.5,    # 学习率\n",
    "    max_depth=12,           # 树的最大深度\n",
    "    subsample=0.8,         # 样本采样比例\n",
    "    colsample_bytree=0.8,  # 特征采样比例\n",
    "    objective='reg:squarederror',  # 回归任务\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    #early_stopping_rounds=50,  # 早停法防止过拟合\n",
    "    verbose=10                 # 每10轮打印一次日志\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363d8d0-ab7f-49f6-b903-9a9775871e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测验证集\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# 计算误差\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f\"Validation MAE: {mae:.4f}, MSE: {mse:.4f}\")\n",
    "\n",
    "# 自定义得分（根据题目公式）\n",
    "def custom_score(y_true, y_pred):\n",
    "    return np.mean(np.maximum(0, 1 - np.log(1+0.1*abs(y_pred-y_true))/5))\n",
    "\n",
    "score = custom_score(y_val, y_pred)\n",
    "print(f\"Custom Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44bbca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取验证数据\n",
    "if os.environ.get('DATA_PATH'):\n",
    "    data_path = os.environ.get(\"DATA_PATH\") + \"/\"  \n",
    "else:\n",
    "    print(\"Baseline运行时，因为无法读取验证集，所以会有此条报错，属于正常现象\")  #Baseline运行时，因为无法读取验证集，所以会有此条报错，属于正常现象\n",
    "    print(\"When the baseline is running, this error message will appear because the val set cannot be read, which is a normal phenomenon.\") #When the baseline is running, this error message will appear because the val set cannot be read, which is a normal phenomenon.\n",
    "\n",
    "val_df = pd.read_csv(data_path + \"val_data_question.dat\")\n",
    "\n",
    "L2M = val_df.iloc[:, 1].values\n",
    "D = val_df.iloc[:, 2].values\n",
    "L = val_df.iloc[:, 3].values\n",
    "\n",
    "val_df['c(L2M)/c(D)'] = L2M / D\n",
    "val_df['c(L)/c(D)'] = L / D\n",
    "val_df['c(L)/c(L2M)'] = L / L2M\n",
    "val_df['c(L2M)*c(D)'] = L2M * D\n",
    "val_df['c(L)*c(D)'] = L * D\n",
    "val_df['c(L)*c(L2M)'] = L * L2M\n",
    "val_df['L2M_mole'] = L2M / (L2M + D + L)\n",
    "val_df['D_mole'] = D / (L2M + D + L)\n",
    "val_df['L_mole'] = L / (L2M + D + L)\n",
    "val_df['logc(L2M)'] = np.log(1+L2M)\n",
    "\n",
    "X_val = val_df.iloc[:, [1,2,3,4,5,6,7,8,9,10,11,12,13]].values\n",
    "\n",
    "\n",
    "# 归一化（与训练集使用相同的scaler）\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# 预测半衰期\n",
    "t_half_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# 生成提交文件\n",
    "submission_df_val = pd.DataFrame({\n",
    "    \"Experiment #\": val_df[\"Experiment_#\"],\n",
    "    \"t~1/2~\": [\"{:.4e}\".format(x) for x in t_half_pred],  # 科学计数法保留4位小数\n",
    "})\n",
    "\n",
    "submission_df_test.to_csv(\"submission_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828cfaf1-667d-48d4-9f4f-48b1598d198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取测试数据\n",
    "if os.environ.get('DATA_PATH'):\n",
    "    data_path = os.environ.get(\"DATA_PATH\") + \"/\"  \n",
    "else:\n",
    "    print(\"Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\")  #Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\n",
    "    print(\"When the baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\") #When the baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\n",
    "\n",
    "val_df = pd.read_csv(data_path + \"test_data_question.dat\")\n",
    "\n",
    "L2M = test_df.iloc[:, 1].values\n",
    "D = test_df.iloc[:, 2].values\n",
    "L = test_df.iloc[:, 3].values\n",
    "\n",
    "test_df['c(L2M)/c(D)'] = L2M / D\n",
    "test_df['c(L)/c(D)'] = L / D\n",
    "test_df['c(L)/c(L2M)'] = L / L2M\n",
    "test_df['c(L2M)*c(D)'] = L2M * D\n",
    "test_df['c(L)*c(D)'] = L * D\n",
    "test_df['c(L)*c(L2M)'] = L * L2M\n",
    "test_df['L2M_mole'] = L2M / (L2M + D + L)\n",
    "test_df['D_mole'] = D / (L2M + D + L)\n",
    "test_df['L_mole'] = L / (L2M + D + L)\n",
    "test_df['logc(L2M)'] = np.log(1+L2M)\n",
    "\n",
    "X_test = test_df.iloc[:, [1,2,3,4,5,6,7,8,9,10,11,12,13]].values\n",
    "\n",
    "\n",
    "# 归一化（与训练集使用相同的scaler）\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 预测半衰期\n",
    "t_half_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 生成提交文件\n",
    "submission_df_test = pd.DataFrame({\n",
    "    \"Experiment #\": test_df[\"Experiment_#\"],\n",
    "    \"t~1/2~\": [\"{:.4e}\".format(x) for x in t_half_pred],  # 科学计数法保留4位小数\n",
    "})\n",
    "\n",
    "submission_df_test.to_csv(\"submission_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('submission.zip', 'w') as zipf:\n",
    "        zipf.write('submission_val.csv')\n",
    "        zipf.write('submission_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d6016f-0935-4834-9a91-aa245b17d5e6",
   "metadata": {},
   "source": [
    "## Method 2: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "413dbcd0-d80a-436c-8805-7ed11e571816",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_root = '/bohr/train-lmss/v1/data_train/'\n",
    "file_root_p = os.listdir(file_root)\n",
    "file_root_p.sort()\n",
    "\n",
    "column_name = ['L2M_i','D_i','L_i','L2MD_i','L2Ms_i','L2M','D','L','L2MD','L2Ms']\n",
    "data = pd.DataFrame(columns = column_name)\n",
    "\n",
    "for file_name in file_root_p:\n",
    "    if file_name == \".DS_Store\" or file_name == \".ipynb_checkpoints\" or file_name == \"training_data.dat\":\n",
    "        continue\n",
    "    \n",
    "    #print(file_name)\n",
    "    df = pd.read_csv(os.path.join(file_root, file_name))\n",
    "    exp = df.iloc[:, 1:6].values\n",
    "    features = exp[:-1, :]\n",
    "    targets = exp[1:, :]\n",
    "    if data.size == 0:\n",
    "        for i in range(5):\n",
    "            data[column_name[i]] = features[:,i]\n",
    "        for i in range(5):\n",
    "            data[column_name[i+5]] = targets[:,i]\n",
    "    else:\n",
    "        tmp = pd.DataFrame(columns = ['L2M_i','D_i','L_i','L2MD_i','L2Ms_i','L2M','D','L','L2MD','L2Ms'])\n",
    "        for i in range(5):\n",
    "            tmp[column_name[i]] = features[:,i]\n",
    "        for i in range(5):\n",
    "            tmp[column_name[i+5]] = targets[:,i]\n",
    "        data = pd.concat([data, tmp], axis=0, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2981d-e205-430b-90a6-90a1cb9909ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94b87f7e-e5a2-42a1-93ea-daa88e95d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.input = torch.tensor(data.iloc[:, 0:5].values, dtype=torch.float32)\n",
    "        self.target = torch.tensor(data.iloc[:, 5:10].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target.shape[0];\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.input[i], self.target[i];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc93a1d2-2a43-4092-b17e-ded78eb84f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#train, val = train_test_split(data, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f41e4239-94aa-45ff-aee3-15324f19b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 12)\n",
    "        self.fc2 = nn.Linear(12, 24)\n",
    "        self.fc3 = nn.Linear(24, 48)\n",
    "        self.fc4 = nn.Linear(48, 16)\n",
    "\n",
    "        self.heads = nn.ModuleList([nn.Linear(16, 1) for _ in range(5)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = torch.relu(self.fc1(x))\n",
    "        x2 = torch.relu(self.fc2(x1))\n",
    "        x3 = torch.relu(self.fc3(x2))\n",
    "        x4 = torch.relu(self.fc4(x3))\n",
    "\n",
    "        outputs = [head(x4) for head in self.heads]\n",
    "        if outputs[0].shape[0] > 1:\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "        else:\n",
    "            outputs = torch.tensor(outputs, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        return x+outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6472e349-a451-4a1c-9abe-3b652a233432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#setup training config\n",
    "num_epochs = 30\n",
    "LR = 0.00005\n",
    "model = SimpleModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "#pd = pd.DataFrame(columns = ['ind', 'actual', 'predicted'])\n",
    "#ind = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    tot_loss = 0\n",
    "    num = 0\n",
    "    \n",
    "    for inputs, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        tot_loss += loss\n",
    "        num += 1\n",
    "    \n",
    "    avg_loss = tot_loss / num\n",
    "    print(f'epoch {epoch}; loss {avg_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f9b1a31-d373-4033-a487-0773d5d29f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t21_simulate(model, c_initial, dt, num_steps):\n",
    "    input_tensor = torch.tensor(c_initial, dtype=torch.float32)\n",
    "    #print(input_tensor)\n",
    "    #print(input_tensor.size())\n",
    "    predictions = []\n",
    "    step_num = 0\n",
    "    predictions.append((step_num * dt, c_initial))\n",
    "    t21_found = 0\n",
    "    with torch.no_grad():\n",
    "        for step in range(num_steps):\n",
    "            step_num += 1\n",
    "            output = model(input_tensor)\n",
    "            predictions.append((step_num * dt, output.squeeze(0).numpy()))\n",
    "            c_L2M_t = output.squeeze(0).numpy()[0]\n",
    "            if c_L2M_t < c_initial[0] / 2.0:\n",
    "                #print(f\"Finished @ t12 = {step_num * dt}\\n\")\n",
    "                t21_found = step_num * dt\n",
    "                break\n",
    "            \n",
    "            input_tensor = output  # Use the output as the input for the next step\n",
    "    predictions_array = np.array([np.hstack((time, conc)) for time, conc in predictions])\n",
    "    predictions_df = pd.DataFrame(predictions_array, columns=['Time', 'c_L2M', 'c_D', 'c_L', 'c_L2MD', 'c_L2Ms'])\n",
    "    return predictions_df, t21_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0cc6f4-00fb-4749-bc7c-29e248d1c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Initial concentrations\n",
    "c_L2MD_0 = 0.0\n",
    "c_L2Ms_0 = 0.0\n",
    "dt = 10.0\n",
    "num_steps = 14400\n",
    "\n",
    "\n",
    "\n",
    "#Read data file\n",
    "if os.environ.get('DATA_PATH'):\n",
    "    data_path = os.environ.get(\"DATA_PATH\") + \"/\"  \n",
    "else:\n",
    "    print(\"Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\")  #Baseline运行时，因为无法读取测试集，所以会有此条报错，属于正常现象\n",
    "    print(\"When the baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\") #When the baseline is running, this error message will appear because the test set cannot be read, which is a normal phenomenon.\n",
    "\n",
    "test_df = pd.read_csv(data_path + \"test_data_question.dat\")\n",
    "\n",
    "input_columns = [1, 2, 3]\n",
    "initial_c = test_df.iloc[:, input_columns].values\n",
    "\n",
    "\n",
    "model.eval()\n",
    "num_exp = initial_c.shape[0]\n",
    "pd_subm = pd.DataFrame(columns = ['Exp #', 't12_simulated'])\n",
    "with torch.no_grad():\n",
    "    for experi in range(num_exp):\n",
    "        c0 = np.concatenate((initial_c[experi], [c_L2MD_0, c_L2Ms_0]))\n",
    "        _, t21_simulated = t21_simulate(model, c0, dt, num_steps)\n",
    "        pd_subm.loc[experi, 'Exp #']= experi\n",
    "        pd_subm.loc[experi, 't12_simulated'] = t21_simulated\n",
    "\n",
    "\n",
    "\n",
    "pd_subm['t12_simulated'] = pd_subm['t12_simulated'].apply(lambda x: f\"{x:.4e}\")\n",
    "pd_subm.to_csv('submission_test.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5126ca3-6179-449a-bc3d-d3eee4f14bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Initial concentrations\n",
    "c_L2MD_0 = 0.0\n",
    "c_L2Ms_0 = 0.0\n",
    "dt = 10.0\n",
    "num_steps = 14400\n",
    "\n",
    "\n",
    "\n",
    "#Read data file\n",
    "if os.environ.get('DATA_PATH'):\n",
    "    data_path = os.environ.get(\"DATA_PATH\") + \"/\"  \n",
    "else:\n",
    "    print(\"Baseline运行时，因为无法读取验证集，所以会有此条报错，属于正常现象\")  #Baseline运行时，因为无法读取验证集，所以会有此条报错，属于正常现象\n",
    "    print(\"When the baseline is running, this error message will appear because the val set cannot be read, which is a normal phenomenon.\") #When the baseline is running, this error message will appear because the val set cannot be read, which is a normal phenomenon.\n",
    "\n",
    "val_df = pd.read_csv(data_path + \"val_data_question.dat\")\n",
    "\n",
    "input_columns = [1, 2, 3]\n",
    "initial_c = val_df.iloc[:, input_columns].values\n",
    "\n",
    "\n",
    "model.eval()\n",
    "num_exp = initial_c.shape[0]\n",
    "pd_subm = pd.DataFrame(columns = ['Exp #', 't12_simulated'])\n",
    "with torch.no_grad():\n",
    "    for experi in range(num_exp):\n",
    "        c0 = np.concatenate((initial_c[experi], [c_L2MD_0, c_L2Ms_0]))\n",
    "        _, t21_simulated = t21_simulate(model, c0, dt, num_steps)\n",
    "        pd_subm.loc[experi, 'Exp #']= experi\n",
    "        pd_subm.loc[experi, 't12_simulated'] = t21_simulated\n",
    "\n",
    "\n",
    "\n",
    "pd_subm['t12_simulated'] = pd_subm['t12_simulated'].apply(lambda x: f\"{x:.4e}\")\n",
    "pd_subm.to_csv('submission_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('submission.zip', 'w') as zipf:\n",
    "        zipf.write('submission_val.csv')\n",
    "        zipf.write('submission_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "a2656735f91d6ebcd4e80a7c772d2d00464e1be694f70c1638feaa40b374f934"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
